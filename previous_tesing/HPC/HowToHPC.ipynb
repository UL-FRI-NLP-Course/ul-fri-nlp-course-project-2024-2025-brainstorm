{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47fff8f4",
   "metadata": {},
   "source": [
    "# Naš skuppern folder je \n",
    "\n",
    "```bash \n",
    "cd /d/hpc/projects/onj_fri/brainstorm\n",
    "```\n",
    "pol pa ce uporablas vs code (bols da uprabla vs code) \n",
    "napis to da se ti odpre vs code direkt tm ... sam laži je k da vse delas v terminalu\n",
    "```bash\n",
    "code .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf9f57",
   "metadata": {},
   "source": [
    "### ce hocta se kej infota je v NPL-Course-tuturoal \n",
    "\n",
    "12- llm\n",
    "run_rag_local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651a5ce",
   "metadata": {},
   "source": [
    "# ce hoces runat py kodo nared nek .sh file \n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=LLM-model-test    # Job name\n",
    "#SBATCH --partition=gpu              # Partition (queue) name\n",
    "#SBATCH --nodes=1                    # Number of nodes\n",
    "#SBATCH --ntasks=1                   # Number of tasks (processes)\n",
    "#SBATCH --cpus-per-task=1            # CPU cores/threads per task\n",
    "#SBATCH --gres=gpu:1                 # Number of GPUs per node\n",
    "#SBATCH --mem=16G                    # Job memory request (increased for LLM)\n",
    "#SBATCH --time=02:00:00              # Time limit hrs:min:sec\n",
    "\n",
    "# Standard output and error log\n",
    "#SBATCH --output=logs/llm-model-test-%J.out\n",
    "#SBATCH --error=logs/llm-model-test-%J.err\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "mkdir -p logs\n",
    "\n",
    "# Run the model test script in the container\n",
    "singularity exec --nv --overlay container/overlay2.img container/container_llm2.sif python3 model_test.py\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2a0d7f",
   "metadata": {},
   "source": [
    "Pol k hoces za runat najprej \n",
    "```bash \n",
    "\n",
    "chmod +x slurm.sh\n",
    "sbatch slurm.sh\n",
    "```\n",
    "pa ce hoces pogledat ti pol izpise nek id tvojga taska k si ga zarnu in po loh tko pogledas kaj se dogaja z njim \n",
    "```bash\n",
    "# Get current global queue\n",
    "squeue\n",
    "\n",
    "# Get the queue of your jobs only\n",
    "squeue -u szitnik\n",
    "\n",
    "# Get an (rough) estimation of starting your job\n",
    "squeue -j <jobid> --start\n",
    "\n",
    "# Get basic information of a waiting job\n",
    "sacct -j <jobid>\n",
    "\n",
    "# Get some useful statistics of your running job (CPU, MEM, ...)\n",
    "sstat -j <jobid>\n",
    "\n",
    "# Cancel a job\n",
    "scancel <jobid>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb64eb90",
   "metadata": {},
   "source": [
    "# Kako nardit container\n",
    "```bash\n",
    "singularity build container_llm2.sif container_llm2.def\n",
    "```\n",
    "\n",
    "## dodatki kako dodat lib ce ga ni \n",
    "```bash \n",
    "singularity overlay create --size 1024 container/overlay2.img\n",
    "\n",
    "singularity shell --overlay container/overlay2.img container/container_llm2.sif\n",
    "Singularity> pip3 install faiss-gpu\n",
    "Singularity> exit\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69a417",
   "metadata": {},
   "source": [
    "# file sistem\n",
    "\n",
    "- containder/ \n",
    "    - mejmo containerje to je tko k conda environment ra runta py \n",
    "- logs/\n",
    "    - logi od slurm jobov\n",
    "    - lahko organisamo da nebi sam en velik folder\n",
    "- models/\n",
    "    - model folderi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664281c",
   "metadata": {},
   "source": [
    "# če hočeta zarunat test primer ki sm ga naredu\n",
    "```bash \n",
    "sbatch run_model_test.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7480f",
   "metadata": {},
   "source": [
    "# how i dowload model \n",
    "npr\n",
    "```bash\n",
    "singularity shell --overlay container/overlay2.img container/container_llm2.sif\n",
    "\n",
    "\n",
    "huggingface-cli login\n",
    "\n",
    "\n",
    "python3 -c \"from huggingface_hub import snapshot_download; snapshot_download(repo_id='meta-llama/Llama-4-Maverick-17B-128E-Instruct', local_dir='models/Llama-4-Maverick-17B-128E-Instruct', local_dir_use_symlinks=False)\"\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
