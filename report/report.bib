@article{Demsar2016BalancedMixture,
    title = {{A Balanced Mixture of Antagonistic Pressures Promotes the Evolution of Parallel Movement}},
    year = {2016},
    journal = {Scientific Reports},
    author = {Dem{\v{s}}ar, Jure and {\v{S}}trumbelj, Erik and Lebar Bajec, Iztok},
    volume = {6},
    doi = {10.1038/srep39428}
}

@article{Demsar2017LinguisticEvolution,
    title = {{Evolution of Collective Behaviour in an Artificial World Using Linguistic Fuzzy Rule-Based Systems}},
    year = {2017},
    journal = {PLoS ONE},
    author = {Dem{\v{s}}ar, Jure and Lebar Bajec, Iztok},
    number = {1},
    pages = {1--20},
    volume = {12},
    doi = {10.1371/journal.pone.0168876}
}


@misc{cercola2025automatinglooptrafficincident,
      title={Automating the loop in traffic incident management on highway}, 
      author={Matteo Cercola and Nicola Gatti and Pedro Huertas Leyva and Benedetto Carambia and Simone Formentin},
      year={2025},
      eprint={2503.12085},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2503.12085}, 
}

@misc{vreš2024generativemodellessresourcedlanguage,
      title={Generative Model for Less-Resourced Language with 1 billion parameters}, 
      author={Domen Vreš and Martin Božič and Aljaž Potočnik and Tomaž Martinčič and Marko Robnik-Šikonja},
      year={2024},
      eprint={2410.06898},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.06898}, 
}

@misc{zhu2024multilingualmachinetranslationlarge,
      title={Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis}, 
      author={Wenhao Zhu and Hongyi Liu and Qingxiu Dong and Jingjing Xu and Shujian Huang and Lingpeng Kong and Jiajun Chen and Lei Li},
      year={2024},
      eprint={2304.04675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.04675}, 
}

@misc{pelofske2024automatedmultilanguageenglishmachine,
      title={Automated Multi-Language to English Machine Translation Using Generative Pre-Trained Transformers}, 
      author={Elijah Pelofske and Vincent Urias and Lorie M. Liebrock},
      year={2024},
      eprint={2404.14680},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14680}, 
}

@article{articleRTTRS,
author = {Wan, Xiangpeng and Lucic, Michael and Ghazzai, Hakim and Massoud, Yehia},
year = {2020},
month = {01},
pages = {159-175},
title = {Empowering Real-Time Traffic Reporting Systems With NLP-Processed Social Media Data},
volume = {1},
journal = {IEEE Open Journal of Intelligent Transportation Systems},
doi = {10.1109/OJITS.2020.3024245}
}

@misc{j2024finetuningllmenterprise,
      title={Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations}, 
      author={Mathav Raj J and Kushala VM and Harikrishna Warrier and Yogesh Gupta},
      year={2024},
      eprint={2404.10779},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2404.10779}, 
}


@misc{white2023promptpatterncatalogenhance,
      title={A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT}, 
      author={Jules White and Quchen Fu and Sam Hays and Michael Sandborn and Carlos Olea and Henry Gilbert and Ashraf Elnashar and Jesse Spencer-Smith and Douglas C. Schmidt},
      year={2023},
      eprint={2302.11382},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2302.11382}, 
}

@misc{Data2Text,
  author       = {Microsoft Research},
  title        = {Data2Text: Automated Text Generation from Structured Data},
  year         = {2025},
  url          = {https://www.microsoft.com/en-us/research/project/data2text-automated-text-generation-from-structured-data/},
  note         = {Accessed: 2025-03-19}
}


@misc{pelofske2024automatedmultilanguageenglishmachine,
      title={Automated Multi-Language to English Machine Translation Using Generative Pre-Trained Transformers}, 
      author={Elijah Pelofske and Vincent Urias and Lorie M. Liebrock},
      year={2024},
      eprint={2404.14680},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14680}, 
}


@misc{LlamaIndex2025,
  author       = {LlamaIndex},
  title        = {Building Blocks of LLM Report Generation: Beyond Basic RAG},
  year         = {2025},
  url          = {https://www.llamaindex.ai/blog/building-blocks-of-llm-report-generation-beyond-basic-rag},
  note         = {Accessed: 2025-03-19}
}


@InProceedings{10.1007/978-3-031-70546-5_9,
author="Lamott, Marcel
and Weweler, Yves-Noel
and Ulges, Adrian
and Shafait, Faisal
and Krechel, Dirk
and Obradovic, Darko",
editor="Barney Smith, Elisa H.
and Liwicki, Marcus
and Peng, Liangrui",
title="LAPDoc: Layout-Aware Prompting for Documents",
booktitle="Document Analysis and Recognition - ICDAR 2024",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="142--159",
abstract="Recent advances in training large language models (LLMs) using massive amounts of solely textual data lead to strong generalization across many domains and tasks, including document-specific ones. On the other hand, there is a trend to train multi-modal transformer architectures tailored for document understanding that are designed specifically to fuse textual inputs with the corresponding document layout. This involves a separate fine-tuning step for which additional training data is required. At present, no document transformers with comparable generalization to LLMs are available. This raises the question which type of model is to be preferred for document understanding tasks. In this paper we investigate the possibility to use purely text-based LLMs for document-specific tasks by using layout enrichment. We explore drop-in modifications and rule-based methods to enrich purely textual LLM prompts with layout information. In our experiments we investigate the effects on the commercial ChatGPT model and the open-source LLM Solar. We demonstrate that using our approach both LLMs show improved performance on various standard document benchmarks. In addition, we study the impact of noisy OCR and layout errors, as well as the limitations of LLMs when it comes to utilizing document layout. Our results indicate that layout enrichment can improve the performance of purely text-based LLMs for document understanding by up to 15{\%}, and by 6{\%} on average compared to just using plain document text. In conclusion, this approach should be considered for the best model choice between text-based LLM or multi-modal document transformers.",
isbn="978-3-031-70546-5"
}

@misc{j2024finetuningllmenterprise,
      title={Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations}, 
      author={Mathav Raj J and Kushala VM and Harikrishna Warrier and Yogesh Gupta},
      year={2024},
      eprint={2404.10779},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2404.10779}, 
}


@misc{peng2024automaticnewsgenerationfactchecking,
      title={Automatic News Generation and Fact-Checking System Based on Language Processing}, 
      author={Xirui Peng and Qiming Xu and Zheng Feng and Haopeng Zhao and Lianghao Tan and Yan Zhou and Zecheng Zhang and Chenwei Gong and Yingqiao Zheng},
      year={2024},
      eprint={2405.10492},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.10492}, 
}